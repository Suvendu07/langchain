{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_core.documents import Document"
      ],
      "metadata": {
        "id": "sUKzoti97OKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample documents\n",
        "docs = [\n",
        "    Document(page_content=\"LangChain makes it easy to work with LLMs.\"),\n",
        "    Document(page_content=\"LangChain is used to build LLM based applications.\"),\n",
        "    Document(page_content=\"Chroma is used to store and search document embeddings.\"),\n",
        "    Document(page_content=\"Embeddings are vector representations of text.\"),\n",
        "    Document(page_content=\"MMR helps you get diverse results when doing similarity search.\"),\n",
        "    Document(page_content=\"LangChain supports Chroma, FAISS, Pinecone, and more.\"),\n",
        "]"
      ],
      "metadata": {
        "id": "8uL8bET570ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# Initialize OpenAI embeddings\n",
        "embedding_model = OpenAIEmbeddings()\n",
        "\n",
        "# Step 2: Create the FAISS vector store from documents\n",
        "vectorstore = FAISS.from_documents(\n",
        "    documents=docs,\n",
        "    embedding=embedding_model\n",
        ")"
      ],
      "metadata": {
        "id": "TM7GvqKeA8ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable MMR in the retriever\n",
        "retriever = vectorstore.as_retriever(\n",
        "    search_type=\"mmr\",                   # <-- This enables MMR\n",
        "    search_kwargs={\"k\": 3, \"lambda_mult\": 0.5}  # k = top results, lambda_mult = relevance-diversity balance\n",
        ")"
      ],
      "metadata": {
        "id": "ueDg6SApBAqt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = \"What is langchain?\"\n",
        "results = retriever.invoke(query)"
      ],
      "metadata": {
        "id": "52AkKNz7Chcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, doc in enumerate(results):\n",
        "    print(f\"\\n--- Result {i+1} ---\")\n",
        "    print(doc.page_content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-HvthKyCkBL",
        "outputId": "6bec200b-9541-480f-fa94-1b8c6b3174f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Result 1 ---\n",
            "LangChain is used to build LLM based applications.\n",
            "\n",
            "--- Result 2 ---\n",
            "Embeddings are vector representations of text.\n",
            "\n",
            "--- Result 3 ---\n",
            "LangChain supports Chroma, FAISS, Pinecone, and more.\n"
          ]
        }
      ]
    }
  ]
}